# ğŸš Gesture-Based Drone Control System

---

## ğŸ‡°ğŸ‡· í”„ë¡œì íŠ¸ ìš”ì•½ (Korean Summary)

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì›¹ìº  ì˜ìƒì—ì„œ ì† ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í•˜ê³ ,  
ì†ê°€ë½ ê°œìˆ˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ë“œë¡ ì˜ ë¬¼ë¦¬ì  ë™ì‘(ì´ë¥™, ì´ë™, ì°©ë¥™)ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.  

MediaPipe ê¸°ë°˜ ì† ëœë“œë§ˆí¬ ì¸ì‹ê³¼ CoDrone Miniì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬  
ì‹¤ì‹œê°„ ë¹„ì „ ì¸ì‹ â†’ ì•ˆì •ì„± íŒë‹¨ â†’ í•˜ë“œì›¨ì–´ ì œì–´ê¹Œì§€ ì „ì²´ íë¦„ì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.  

ë‹¨ìˆœ ì œìŠ¤ì²˜ ì¸ì‹ êµ¬í˜„ì„ ë„˜ì–´ì„œ,  
ì‹¤ì œ ë¹„í–‰ ì‹¤í—˜ì„ í†µí•´ ë°œìƒí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì— ì¤‘ì ì„ ë‘” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

## ğŸ“Œ Project Overview

This project implements a real-time gesture recognition system  
that controls a drone using hand gestures detected from a webcam.

Hand landmarks are extracted using MediaPipe,  
and the number of extended fingers (0â€“5) is mapped to drone commands.

Rather than focusing only on gesture recognition,  
this project emphasizes:

- **system-level integration**
- motion stability
- hardware-aware control tuning
- safe initialization and execution logic

---

## ğŸ§  Gesture Recognition Pipeline

The system processes the camera input as follows:

### ğŸ”„ Overall Pipeline

Camera â†’ Hand Landmark Detection â†’ Finger Counting  
â†’ Stable Gesture Confirmation â†’ Drone Mission Execution

---

### 1. Camera Input (OpenCV)

- Captures real-time webcam frames  
- Converts BGR to RGB for MediaPipe processing  

---

### 2. Hand Landmark Detection (MediaPipe)

- Detects 21 hand landmarks  
- Tracks hand movement in real-time  
- Supports dynamic gesture recognition  

---

### 3. Finger Counting Logic

- Index / Middle / Ring / Pinky  
  â†’ Compare fingertip y-coordinate with lower joint  

- Thumb  
  â†’ Distance-based comparison between thumb tip and pinky base  
  â†’ Improves robustness against palm/back orientation differences  

---

### 4. Stability Confirmation

To avoid unintended drone motion:

- The same gesture must be maintained for **1 second**
- After execution, the hand must be lowered before the next command
- Edge-trigger structure prevents repeated execution

This significantly improves real-world safety and control reliability.

---

## ğŸš Drone Motion Control Design

Real-world flight testing revealed several control challenges.

To improve motion stability, the following structure was implemented:

### ğŸ”§ Control Structure

- `control()` â†’ raw directional control  
- `brake()` â†’ short reverse thrust to remove inertia  
- `hover()` â†’ trim-based stabilization  

### ğŸ”„ Movement Pattern

Move â†’ Brake â†’ Hover

This pattern reduces drift and overshoot after movement.

---

## ğŸ”§ Experimental Findings & Improvements

### 1ï¸âƒ£ Hover Instability

- Problem: Drone drifted during hover  
- Solution: Applied trim values only in hover phase  

---

### 2ï¸âƒ£ Inertia After Movement

- Problem: Drone continued moving after control input ended  
- Solution: Introduced brake phase before hover  

---

### 3ï¸âƒ£ Unexpected Restart Behavior

- Problem: After collision or disconnection, previous motion resumed  
- Solution: Implemented `safe_initialize()` routine  
  - Repeated landing commands  
  - Control value reset  

---

### 4ï¸âƒ£ Inconsistent Movement Distance

- Identical control values produced different results  
- Cause: Real-time battery drain affects motor power  
- Limitation: No API access to real-time battery compensation  

---

## ğŸ® Gesture-to-Drone Mapping

| Gesture | Action |
|----------|--------|
| 1 | Takeoff |
| 2 | Forward |
| 3 | Backward |
| 4 | Left |
| 5 | Right |
| 0 | Landing |

---

## ğŸ”§ Implementation Details

- **Language**: Python  
- **Libraries**:
  - OpenCV (camera processing)
  - MediaPipe (hand landmark detection)
  - e_drone (CoDrone Mini control SDK)
  - NumPy (mathematical operations)

- **Environment**:
  - Conda virtual environment
  - Bluetooth communication with CoDrone Mini

---

## ğŸ“‚ Code Structure & Development Stages

The following table summarizes the development stages and the role of each script used in this project.

| ë‹¨ê³„ | ì£¼ìš” íŒŒì¼ | ì—­í•  |
|------|----------|------|
| ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸ | `camera_test.py` | ì›¹ìº  ë™ì‘ í™•ì¸ |
| ì† ì¸ì‹ ë””ë²„ê¹… | `hand_debug.py` | ì† ëœë“œë§ˆí¬ ì‹œê°í™” (MediaPipe) |
| ì œìŠ¤ì²˜ ì•ˆì •í™” | `gesture_stable_command.py` | ì†ê°€ë½ ê°œìˆ˜ â†’ ìˆ«ì ê³„ì‚° |
| ë“œë¡  ì œì–´ ì‹¤í—˜ | `drone_basic_test.py` | ì´ë™ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ë° íŠœë‹ |
| ë“œë¡  ë¯¸ì…˜ ì„¤ê³„ | `drone_missions.py` | ìˆ«ìë³„ ë“œë¡  ë™ì‘ ë§¤í•‘ |
| í†µí•© ì œì–´ | `main_gesture_to_drone.py` | ì œìŠ¤ì²˜ â†’ ë“œë¡  ì œì–´ í†µí•© ì‹œìŠ¤í…œ êµ¬í˜„|

---

## âš ï¸ Limitations

- Flight time limited (~5 minutes)  
- Battery drain affects movement consistency    
- Motion precision limited by available sensors  

---

## ğŸ¯ Design Philosophy

- âŒ Autonomous navigation  
- âŒ Complex control theory  
- â­• Real-time vision-to-hardware integration  
- â­• Stable execution logic  
- â­• Parameter tuning through real flight experiments  
- â­• Safety-first system design  

This project demonstrates how computer vision can be integrated with physical drone control,  
with a strong focus on real-world experimentation and reliability.
